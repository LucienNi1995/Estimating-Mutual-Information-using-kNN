# Estimating-Mutual-Information-using-kNN
The project is focused on numerically estimating entropy and mutual information using k-th nearest neighbor estimators and its applications in related areas. Entropy and mutual information are defined as follows:

$$H(x):=-\mathbb{E}[\log{(\mu_x(x))}]$$

I(x,y):=H(x)+H(y)-H(x,y)

For continuous estimators, KSG, BI-KSG and G-knn estimators were reproduced. For discrete case, Gaoâ€™s estimator and Multi-KL estimator were reproduced. 
