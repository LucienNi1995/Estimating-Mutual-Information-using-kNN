# Estimating-Mutual-Information-using-kNN
The project is focused on numerically estimating entropy and mutual information using k-th nearest neighbor estimators and its applications in related areas. Entropy and mutual information are defined as follows:

H(x):=-E[log(mu_x(x))]

I(x,y):=H(x)+H(y)-H(x,y)

For continuous estimators, KSG, BI-KSG and G-knn estimators were reproduced. For discrete case, Gao’s estimator and Multi-KL estimator were reproduced. 


Reference:
1. A. Kraskov, H. St ̈ogbauer, and P. Grassberger. “Estimating mutual information”. In: Phys. Rev.
E 69.6 (2004).

2. B. C. Ross. “Mutual Information between Discrete and Continuous Data Sets”. In: PLOS ONE
9.2 (Feb. 2014), pp. 1–5. doi: 10.1371/journal.pone.0087357. url: https://doi.org/10.1371/
journal.pone.0087357.

3. W. Gao, S. Oh, and P. Viswanath. “Demystifying fixed k-nearest neighbor information estima-
tors”. In: IEEE International Symposium on Information Theory (ISIT) (2017), pp. 1267–1271.

4. W. M. Lord, J. Sun, and E. M. Bollt. “Geometric k-nearest neighbor estimation of entropy and
mutual information”. In: Chaos: An Interdisciplinary Journal of Nonlinear Science 28.3 (2018).

5. S. Gao, G. V. Steeg, and A. Galstyan. “Efficient Estimation of Mutual Information for Strongly
Dependent Variables”. In: Artificial intelligence and statistics (2015), pp. 277–286.
